{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a956c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f6c5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codebook(nn.Module):\n",
    "    def __init__(self, n_codes, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embeddings', torch.randn(n_codes, embedding_dim))\n",
    "        self.register_buffer('N', torch.zeros(n_codes))\n",
    "        self.register_buffer('z_avg', self.embeddings.data.clone())\n",
    "\n",
    "        self.n_codes = n_codes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self._need_init = True\n",
    "\n",
    "    def _tile(self, x):\n",
    "        d, ew = x.shape\n",
    "        if d < self.n_codes:\n",
    "            n_repeats = (self.n_codes + d - 1) // d\n",
    "            std = 0.01 / np.sqrt(ew)\n",
    "            x = x.repeat(n_repeats, 1)\n",
    "            x = x + torch.randn_like(x) * std\n",
    "        return x\n",
    "\n",
    "    def _init_embeddings(self, z):\n",
    "        # z: [b, c, t, h, w]\n",
    "        self._need_init = False  # 只初始化第一次\n",
    "        flat_inputs = shift_dim(z, 1, -1).flatten(end_dim=-2)  # [b,t,h,w,c]->[b*t*h*w, c]\n",
    "        y = self._tile(flat_inputs)\n",
    "\n",
    "        d = y.shape[0]\n",
    "        # [n_codes, embed_dim], integer. 随机选择n_codes个作为embedding的初始值\n",
    "        # 保证在最初的mapping时，每个embedding vector都尽量被用到？\n",
    "        \n",
    "        _k_rand = y[torch.randperm(y.shape[0])][:self.n_codes] \n",
    "        \n",
    "        if dist.is_initialized():\n",
    "            dist.broadcast(_k_rand, 0)\n",
    "        self.embeddings.data.copy_(_k_rand)\n",
    "        self.z_avg.data.copy_(_k_rand)\n",
    "        self.N.data.copy_(torch.ones(self.n_codes))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [b, c, t, h, w], z_e(x), encoder output\n",
    "        if self._need_init and self.training:\n",
    "            self._init_embeddings(z)\n",
    "        flat_inputs = shift_dim(z, 1, -1).flatten(end_dim=-2) # [b,t,h,w,c]->[b*t*h*w,c]\n",
    "        distances = (flat_inputs ** 2).sum(dim=1, keepdim=True) \\\n",
    "                    - 2 * flat_inputs @ self.embeddings.t() \\\n",
    "                    + (self.embeddings.t() ** 2).sum(dim=0, keepdim=True)\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1) # [b*t*h*w]\n",
    "        encode_onehot = F.one_hot(encoding_indices, self.n_codes).type_as(flat_inputs) # [b*t*h*w, n_codes]\n",
    "        encoding_indices = encoding_indices.view(z.shape[0], *z.shape[2:]) # [b, t,h,w]\n",
    "\n",
    "        # [b, t, h, w, c] 根据indices获取对应的embedding, z_q(x)\n",
    "        embeddings = F.embedding(encoding_indices, self.embeddings) \n",
    "        embeddings = shift_dim(embeddings, -1, 1)  # [b, c, t, h, w]\n",
    "\n",
    "        commitment_loss = 0.25 * F.mse_loss(z, embeddings.detach()) # 将encoder output与对应的embedding拉近\n",
    "\n",
    "        # EMA codebook update\n",
    "        if self.training:\n",
    "            n_total = encode_onehot.sum(dim=0) # [b*t*h*w, n_codes] -> [n_codes]，每个codes被选中的数量\n",
    "            print(\"n_total: \", n_total.shape, n_total)\n",
    "            # 这一步很关键，就是把新得到的 flat_inputs（也就是encoder_output），重新赋予到encoder_sum中\n",
    "            # 这个过程就是index对应的flat_inputs中的vector进行累加\n",
    "            \n",
    "            encode_sum = flat_inputs.t() @ encode_onehot  # [c, n_codes]\n",
    "            print(\"encode_sum: \", encode_sum.shape)\n",
    "            \n",
    "            if dist.is_initialized():\n",
    "                dist.all_reduce(n_total)\n",
    "                dist.all_reduce(encode_sum)\n",
    "\n",
    "            self.N.data.mul_(0.99).add_(n_total, alpha=0.01) # 每个codes被选中的数量的滑动平均值\n",
    "            self.z_avg.data.mul_(0.99).add_(encode_sum.t(), alpha=0.01) # [n_coders, c] embedding的滑动平均值\n",
    "\n",
    "            n = self.N.sum() # 所有codes的总数\n",
    "            weights = (self.N + 1e-7) / (n + self.n_codes * 1e-7) * n # 这算的还是每个codes的滑动平均值呀\n",
    "            print(\"weights: \", weights)\n",
    "            print(\"n:\", n)\n",
    "            print(\"self.N: \", self.N)\n",
    "            \n",
    "            encode_normalized = self.z_avg / weights.unsqueeze(1) # 除以每个codes被用过的次数，就是归一化的embedding\n",
    "            self.embeddings.data.copy_(encode_normalized)  # 到这里是不是就结束了，下面的是在干嘛？？？？\n",
    "\n",
    "            y = self._tile(flat_inputs)\n",
    "            _k_rand = y[torch.randperm(y.shape[0])][:self.n_codes]\n",
    "            if dist.is_initialized():\n",
    "                dist.broadcast(_k_rand, 0)\n",
    "\n",
    "            usage = (self.N.view(self.n_codes, 1) >= 1).float()\n",
    "            self.embeddings.data.mul_(usage).add_(_k_rand * (1 - usage))\n",
    "\n",
    "        embeddings_st = (embeddings - z).detach() + z # stright-through\n",
    "\n",
    "        avg_probs = torch.mean(encode_onehot, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return dict(embeddings=embeddings_st, encodings=encoding_indices,\n",
    "                    commitment_loss=commitment_loss, perplexity=perplexity)\n",
    "\n",
    "    def dictionary_lookup(self, encodings):\n",
    "        embeddings = F.embedding(encodings, self.embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "705d155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n",
    "    n_dims = len(x.shape)\n",
    "    if src_dim < 0:\n",
    "        src_dim = n_dims + src_dim\n",
    "    if dest_dim < 0:\n",
    "        dest_dim = n_dims + dest_dim\n",
    "\n",
    "    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims\n",
    "\n",
    "    dims = list(range(n_dims))\n",
    "    del dims[src_dim]\n",
    "\n",
    "    permutation = []\n",
    "    ctr = 0\n",
    "    for i in range(n_dims):\n",
    "        if i == dest_dim:\n",
    "            permutation.append(src_dim)\n",
    "        else:\n",
    "            permutation.append(dims[ctr])\n",
    "            ctr += 1\n",
    "    x = x.permute(permutation)\n",
    "    if make_contiguous:\n",
    "        x = x.contiguous()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c2fc4",
   "metadata": {},
   "source": [
    "### init embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86bed54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total:  torch.Size([2000]) tensor([ 7.,  1.,  8.,  ..., 10., 41.,  5.])\n",
      "encode_sum:  torch.Size([128, 2000])\n",
      "weights:  tensor([1.0600, 1.0000, 1.0700,  ..., 1.0900, 1.4000, 1.0400])\n",
      "n: tensor(2184.8000)\n",
      "self.N:  tensor([1.0600, 1.0000, 1.0700,  ..., 1.0900, 1.4000, 1.0400])\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(5, 128, 4, 32, 32)\n",
    "codebook = Codebook(2000, 128)\n",
    "out = codebook(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b8b9a",
   "metadata": {},
   "source": [
    "### some details explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a359232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 8, 4, 5, 0, 7, 1, 9, 3, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b1b911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.LongTensor([1,2,3,3,2])\n",
    "encode_onehot = F.one_hot(indices, 5)\n",
    "encode_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33b0b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_inputs = torch.ones(5, 8)  # bs, embed_dim[]\n",
    "flat_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe57af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flat_inputs.t() @ encode_onehot.float()).t()  # [c_codes, embed_dim]\n",
    "# index对应的vector进行累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74113fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt1.7] *",
   "language": "python",
   "name": "conda-env-pt1.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
